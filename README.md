# Instruction Tuning GPT2 on Alpaca Dataset
- Author: Sovit Ranjan Rath _ May 6, 2024
- Practice: Mr. Jack _ May30, 2024
- Link: https://debuggercafe.com/instruction-tuning-gpt2-on-alpaca-dataset/

Fine-tuning language models to follow instructions is a major step in making them more useful. In this article, we will train the GPT2 model for following simple instructions. Instruction tuning GPT2 on the Alpaca dataset will reveal how well very small language models perform at following instructions.

In particular, we will train the GPT2 base model which contains just 124 million parameters. This is much smaller than what the industry considers as SLMs (Small Language Models), which us typically 7 bllion (7B) parameters. In fact, any language model below 3 billion parameters can be a challenge to to train for instruction following. However, in future posts, we will train many such models and see how far we can push the envelope forward. This post is a starting point for this.

~> HÃ´m qua (29 May 2024) tÃ¬m tháº¥y Ä‘Æ°á»£c má»™t chá»§ Ä‘á» hay nÃªn pháº£i báº¯t tay vÃ o thá»±c hÃ nh ngay, káº¿t quáº£ ráº¥t tuyá»‡t ^^ . Há»“i trÆ°á»›c mÃ¬nh chá»‰ thÃ­ch thuáº­t toÃ¡n AI , khÃ´ng thÃ­ch lÃ m giao diá»‡n (UI/UX), khÃ´ng thÃ­ch lÃ m Data, ... khÃ´ng thÃ­ch nÃ y ná» ... giá» khi lÃ m demo AI lÃ  vÆ°á»›ng toÃ n diá»‡n khÃ´ng chá»«a má»¥c nÃ o, tá»« a-z, nÃªn va vÃ o cÃ¡i nÃ o ngáº¥t luÃ´n cÃ¡i Ä‘Ã³ ğŸ˜‚ ... mÃ¬nh cÅ©ng Ä‘ang pháº£i Ã´n táº­p láº¡i má»™t sá»‘ ná»™i dung cÆ¡ báº£n cá»§a Javascript, UI/UX, Gradio, Dataset ... vÃ¬ náº¿u khÃ´ng thÃ´ng máº¥y cÃ¡i nÃ y mÃ  copy-paste source code cá»§a tÃ¢y vá» thá»­ mÃ  nÃ³ khÃ´ng cháº¡y á»Ÿ má»™t chá»— nÃ o Ä‘Ã³, vÃ¬ má»™t lÃ½ do nÃ o Ä‘Ã³ thÃ¬ chá»‰ cÃ³ ngá»“i khÃ³c báº±ng tiáº¿ng mÃ¡n ğŸ˜‚ ... há»“i trÆ°á»›c mÃ¬nh nghÄ© khÃ¡ Ä‘Æ¡n giáº£n lÃ  open source ráº¥t hay, chá»‰ cáº§n lÃªn github tÃ¬m cÃ¡i nÃ o hay hay tháº¥y thÃ­ch thÃ¬ clone vá» cháº¡y demo Ä‘Æ°á»£c thÃ¬ tá»©c lÃ  mÃ¬nh Ä‘Ã£ há»c xong pháº§n Ä‘Ã³ ^^ khÃ´ng ngá» thá»±c táº¿ lÃ  cÃ i Ä‘áº·t mÃ´i trÆ°á»ng cÅ©ng loay hoay, cháº¡y cÅ©ng khÃ´ng ná»•i, rá»“i cháº¡y cÅ©ng lá»—i khÃ´ng biáº¿t Ä‘Ã¢u mÃ  láº§n ğŸ˜‚ ... váº­y nÃªn tá»« "ná»—i Ä‘au" Ä‘Ã³ mÃ¬nh tá»± Ä‘áº·t ra má»™t nguyÃªn táº¯c lÃ  source code mÃ¬nh pháº£i review cáº©n tháº­n, chá»‰nh sá»­a cho gá»n gÃ ng vÃ  test thá»­ ráº¥t ká»¹, Ä‘áº£m báº£o cháº¡y Ä‘Æ°á»£c thÃ¬ má»›i upload lÃªn github cho bÃ  con thá»­ nghiá»‡m ^^ ... vá»›i cÃ¡ch lÃ m "nghiÃªm tÃºc" nhÆ° váº­y nÃªn má»™t sá»‘ repo cÅ©ng cÃ³ ngÃ y Ä‘áº¡t gáº§n trÄƒm lÆ°á»£t clone ^^ váº­y mÃ  chÆ°a cÃ³ cÃ¡i repo nÃ o Ä‘Æ°á»£c chá»¥c sao Ä‘á»ƒ cÃ²n nháº­n Ä‘Æ°á»£c huy hiá»‡u ^^  ... Ä‘Ãºng lÃ  "Ä‘á»i lÃ  bá»ƒ khá»•" mÃ  ğŸ˜‚ <br><br>

~> Vá»›i example nÃ y, máº·c dÃ¹ mÃ¬nh Ä‘Ã£ copy y nguyÃªn source code cá»§a tÃ¡c giáº£ nhÆ°ng ... váº«n bá»‹ lá»—i :( cháº¯c lÃ  do cÃ i Ä‘áº·t mÃ´i trÆ°á»ng vÃ  cÃ¡c cáº¥u hÃ¬nh chÆ°Æ¡ng trÃ¬nh khÃ¡c nhau bá»‹ lá»—i thÃ nh ra pháº£i fix láº¡i má»™t vÃ i chá»—, sau hÆ¡n 1 ngÃ y lÃ m viá»‡c thÃ¬ cÅ©ng Ä‘Ã£ xá»­ lÃ½ xong ^^ <br>
~> Äá»ƒ thá»ƒ hiá»‡n sá»± tÃ´n trá»ng Ä‘á»‘i vá»›i tÃ¡c giáº£, mÃ¬nh Ä‘á»ƒ nguyÃªn pháº§n code báº£n gá»‘c,  cá»‘ gáº¯ng sá»­a Ã­t nháº¥t cÃ³ thá»ƒ, chá»‰ rÃ o láº¡i vÃ  thÃªm pháº§n code sá»­a láº¡i á»Ÿ ngay dÆ°á»›i Ä‘á»ƒ váº«n Ä‘áº£m báº£o source code cháº¡y Ä‘Æ°á»£c vÃ  ngÆ°á»i Ä‘á»c váº«n follow Ä‘Æ°á»£c pháº§n source code nguyÃªn báº£n ban Ä‘áº§u cá»§a tÃ¡c giáº£ ^^ <br>
~> Trong bÃ i nÃ y tÃ¡c giáº£ dÃ¹ng 'packing=True' Ä‘á»ƒ tá»± Ä‘á»™ng 'concatenate different samples of similar lengths into a single batch' . Tuy nhiÃªn khi cháº¡y thá»±c táº¿ thÃ¬ káº¿t quáº£ tráº£ ra khÃ´ng chÃ­nh xÃ¡c, vÃ¬ váº­y mÃ¬nh Ä‘Ã£ rÃ o chá»— Ä‘Ã³ láº¡i Ä‘á»ƒ máº·c Ä‘á»‹nh setting 'packing=False', nhÆ°ng khi Ä‘Ã³ pháº£i viáº¿t láº¡i preprocess_function(example), cÅ©ng pháº£i loay hoay má»™t lÃºc thÃ¬ má»›i test xong Ä‘Æ°á»£c chá»— nÃ y ^^ . Cuá»‘i cÃ¹ng thÃ¬ example nÃ y cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c "thuáº§n hÃ³a" ^^ <br>

### Tham kháº£o:
- https://huggingface.co/datasets/tatsu-lab/alpaca
- https://huggingface.co/docs
- https://github.com/tatsu-lab/stanford_alpaca
- https://pytorch.org/tutorials/index.html
- https://debuggercafe.com/fine-tuning-phi-1-5-using-qlora
- https://debuggercafe.com/fine-tuning-qwen-1-5-for-coding (All the training and inference shown here were carried out on a system with 10 GB RTX 3080 GPU, 10th generation i7 GPU, and 32 GB of RAM.)
- https://debuggercafe.com/spelling-correction-using-hugging-face-transformers
- https://debuggercafe.com/getting-started-with-grammar-correction
- https://debuggercafe.com/character-level-text-generation-using-lstm
- https://debuggercafe.com/word-level-text-generation-using-lstm
- https://debuggercafe.com/text-generation-with-transformers
- https://debuggercafe.com/introduction-to-gpt-1-and-gpt-2 (Question Answering: ... The smallest GPT-2 model answers less than 1% of the questions correctly even for the most common questions. On the same questions, the largest GPT-2 model answers the question correctly 5.3 times more. This shows that larger models tend to perform better on several zero-shot tasks compared to smaller models with the same architecture.)
- https://skylion007.github.io/OpenWebTextCorpus
- https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

### Screen shot:
![alt text](https://github.com/Mr-Jack-Tung/Instruction-Tuning-GPT2-on-Alpaca-Dataset/blob/main/Screenshot_Instruction-Tuning_GPT2_2024-05-30_01.jpg)

![alt text](https://github.com/Mr-Jack-Tung/Instruction-Tuning-GPT2-on-Alpaca-Dataset/blob/main/Screenshot_Instruction-Tuning_GPT2_2024-05-30_02.jpg)

### Update 31 May 2024: Sá»­ dá»¥ng LoRA trong Instruction Tuning GPT2 ^^
```
from peft import LoraConfig, get_peft_model
peft_config = LoraConfig(
    r=16, # 16, 32, 64, 128
    lora_alpha=32, # 16, 32, 64, 128
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    fan_in_fan_out=True,
    target_modules=[
        "attn.c_attn",
        "attn.c_proj",
        "mlp.c_fc",
        "mlp.c_proj",
    ],
)

model = get_peft_model(model, peft_config)

...
trainer = SFTTrainer(
    ...
    peft_config=peft_config,
)
...
model.merge_adapter()
```
